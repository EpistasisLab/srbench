{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c99d940b",
   "metadata": {},
   "source": [
    "        \n",
    "# Stage 2: Real-world track\n",
    "\n",
    "## Judging Criteria for the Real-World Track\n",
    "\n",
    "\n",
    "For the real-world track, __[Dr. Maimuna Majumder](https://maimunamajumder.com/)__ judged the quality of the SR methods \n",
    "in terms of the models they produce to predict 2-week counts of cases, hospitalizations, and deaths in the COVID-19 pandemic.\n",
    "This assessment carried out by the expert, is subjective and based on their expertise.\n",
    "\n",
    "In more detail, for any given real-world data set:\n",
    "\n",
    "1. Each method ws run 10 times, producing 10 models. \n",
    "Each one of these 10 models is tested on the test set, and the model with median `accuracy` will be considered the representative model for the expert to consider.\n",
    "For this model, `accuracy` and `simplicity`, as per the definitions above, will be reported to the expert for reference.\n",
    "\n",
    "2. The expert will rank the so-obtained competing models in terms of their *trust* in them. \n",
    "Trust is a subjective measure decided by the expert.\n",
    "We will only direct the expert to take into account the level of `accuracy` and `simplicity` to a reasonable extent.\n",
    "The expert is free to interpret these measurements as they please.\n",
    "The expert may, e.g., consider a subjective and not well-defined notion of \"`soundess`\" to be most important.\n",
    "For example, for two models `m_1` and `m_2`, the expert may deem them to be equivalently accurate even though `accuracy_1 > accuracy_2`; moreover, even if `m_1` may have a smaller number of components than `m_2`, the expert may decide that `m_2` is a better model because of the nature of the components in use as well as the way they are combined (e.g., `m_2` contains a realization of the Body Mass Index for a medical problem where the patient's `weight` and `height` are deemed to be important while `m_1` contains unintuive operations, e.g., `atan(log(sqrt(age))/height)`).\n",
    "\n",
    "3. Through discussions it was determined that the best way to rank the methods was by incorporating all three scores, as in stage 1. So, the winning SR method is the one whose model is ranked 1st in terms of the harmonic mean of the expert score, r2_score, and simplicity. Ranks across different data sets are averaged to obtain a final winner.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3055126",
   "metadata": {},
   "source": [
    "# Winners\n",
    "\n",
    "|    | algorithm     |   trust_score |\n",
    "|---:|:--------------|--------------:|\n",
    "|  1 | uDSR          |          5.75 |\n",
    "|  2 | QLattice      |          5.21 |\n",
    "|  3 | geneticengine |          4.99 |\n",
    "|  4 | operon        |          4.8  |\n",
    "|  5 | Bingo         |          4.66 |\n",
    "|  6 | pysr          |          4.17 |\n",
    "|  7 | PS-Tree       |          3.15 |\n",
    "|  8 | E2ET          |          2.72 |\n",
    "|  9 | LassoCV       |          3.45 |\n",
    "\n",
    "*Note: eql runs did not finish successfully and hence it is not rated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f1e4ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from glob import glob\n",
    "from pathlib import Path\n",
    "import sympy as sp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f8b7f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdir = '../results_stage2/'\n",
    "datadir = '../experiment/data/stage2/data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4778c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = []\n",
    "i = 0\n",
    "for f in Path(rdir).rglob('*.json'):\n",
    "#     print(f)\n",
    "    if '7_7' in str(f):\n",
    "        continue\n",
    "    if '135' not in str(f):\n",
    "        continue\n",
    "    if 'gpzgd' in str(f):\n",
    "        continue\n",
    "    with open(f, 'r') as of:\n",
    "        d = json.load(of)\n",
    "    frames.append(d)\n",
    "    i += 1\n",
    "    \n",
    "print('loaded',i,'results')\n",
    "df = pd.DataFrame.from_records(frames)\n",
    "\n",
    "# linear regression is actually lassocv \n",
    "df.loc[df['algorithm']=='LinearRegression','algorithm'] = 'LassoCV'\n",
    "# fix cutoff dataset names\n",
    "df['dataset'] = df['dataset'].apply(lambda x: x+'ata' if x.endswith('_d') else x)\n",
    "########################################\n",
    "# normalize simplicity\n",
    "df['simplicity_original'] = (df['simplicity']-df['simplicity'].min())/(df['simplicity'].max()-df['simplicity'].min())\n",
    "########################################\n",
    "# time transform\n",
    "df['time_hr'] = df['time_time']/3600\n",
    "df['time_mins'] = df['time_time']/60\n",
    "########################################\n",
    "# deconstruct dataset names\n",
    "df['dataset-full'] = df['dataset'].copy()\n",
    "df['dataset'] = df['dataset'].apply(lambda x: '_'.join(x.split('_')[1:-1]))\n",
    "df['task'] = df['dataset'].apply(lambda x: x.split('value_')[-1].split('_')[0])\n",
    "df['horizon'] = df['dataset'].apply(lambda x: 7 if '7' in x else 14)\n",
    "df['random_state'] = df['dataset-full'].apply(lambda x: x.split('_')[0])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6439c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "METRICS = [\n",
    "   'mse_train', \n",
    "   'mae_train', \n",
    "   'r2_train',\n",
    "   'mse_test', \n",
    "   'mae_test', \n",
    "   'r2_test', \n",
    "   'accuracy', \n",
    "   'feature_absence_score' \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6749c9cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.algorithm.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34531f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['task'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a1bb1ba",
   "metadata": {},
   "source": [
    "# check run completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e29af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(['dataset','algorithm'])['random_state'].count().unstack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58146f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "order = df.groupby(['algorithm'])['r2_test'].mean().sort_values(ascending=False).index\n",
    "df.groupby(['task','horizon','algorithm'])['r2_test'].median().unstack().round(3)[order]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a2befed",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(['algorithm'])['r2_test'].median().sort_values(ascending=False).round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb62bc54",
   "metadata": {},
   "outputs": [],
   "source": [
    "order = df.groupby(['algorithm'])['simplicity'].mean().sort_values(ascending=False).index\n",
    "df.groupby(['task','horizon','algorithm'])['simplicity'].mean().unstack().round(3)[order]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f104a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.groupby(['algorithm'])['simplicity'].mean().sort_values(ascending=False).round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff341499",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_sel\n",
    "import pdb\n",
    "median_scores= df.groupby(['task','horizon','algorithm'])['r2_test'].median().unstack()\n",
    "sel_model = {}\n",
    "frames = []\n",
    "for idx, row in median_scores.iterrows():\n",
    "#     task = row['task'], row['horizon'], row['algorithm'], \n",
    "    for alg in row.index:\n",
    "        if np.isnan(row[alg]): continue\n",
    "        dfg = df.loc[\n",
    "            (df.task==idx[0])\n",
    "            & (df.horizon==idx[1])\n",
    "            & (df.algorithm==alg)\n",
    "        ]\n",
    "        entry = dfg.loc[(dfg.r2_test-row.loc[alg]).abs().idxmin()]\n",
    "        assert isinstance(entry, pd.Series)\n",
    "        frames.append(entry.to_dict())\n",
    "df_best = pd.DataFrame.from_records(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a4e588",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_best.loc[df_best.algorithm=='QLattice','symbolic_model']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe9fd78",
   "metadata": {},
   "source": [
    "# evaluate models\n",
    "\n",
    "Note: this was redone from the raw json results due to aggressive rounding in the initial runs. The raw count inputs are quite large, so a larger cutoff was used when rounding floating point numbers in the code below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d61c9cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluation import get_symbolic_model, simplicity, round_floats\n",
    "\n",
    "def redo_model(x):\n",
    "    seed = x['random_state'] #.values[0]\n",
    "    ds = x['dataset'] #.values[0]\n",
    "    task = x['task'] #.values[0]\n",
    "    dataset = pd.read_csv(f'../experiment/data/stage2/data/{seed}_{ds}_train.csv')\n",
    "#     '../experiment/data/stage2/data/'\n",
    "#     print(f'../experiment/data/stage2/data/{seed}_{ds}_{task}_train.csv')\n",
    "    feature_names = [k for k in dataset.columns if k!= task]\n",
    "    local_dict = {k:sp.Symbol(k) for k in feature_names}\n",
    "    if x['algorithm'] == 'QLattice':\n",
    "        print('symbolic_model:',\n",
    "              x.symbolic_model)\n",
    "    mdl = str(get_symbolic_model(x.symbolic_model, local_dict=local_dict, simplify=False))\n",
    "    simp = simplicity(mdl,feature_names,simplify=False)\n",
    "    if x['algorithm'] == 'QLattice':\n",
    "        print('get_symbolic_model:',\n",
    "              mdl)\n",
    "    x['simplified_model'] = round_floats(mdl, 6)\n",
    "    x['simplicity'] = simp\n",
    "#     if x['algorithm'] == 'QLattice': pdb.set_trace()\n",
    "    return x\n",
    "   \n",
    "df_best = df_best.transform(lambda x: redo_model(x), axis=1) \n",
    "# normalize simplicity\n",
    "df_best['simplicity'] = (df_best['simplicity']-df_best['simplicity'].min())/(df_best['simplicity'].max()-df_best['simplicity'].min())\n",
    "\n",
    "df_best[['task','simplified_model','simplicity','algorithm']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f76ef4",
   "metadata": {},
   "source": [
    "# add expert scores\n",
    "\n",
    "Expert scores were determined by a subject expert on COVID-19 after reviewing each model. \n",
    "These scores can be found here: https://docs.google.com/presentation/d/1zVb4HqImP4nB_alrkoQmui16mC8SOxlzZ2uHP3vTltg/edit?usp=sharing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7256bebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "expert_scores = {\n",
    "    'cases': {\n",
    "        'Bingo': 3,\n",
    "        'E2ET': 2,\n",
    "        'LassoCV': 1,\n",
    "        'PS-Tree': 3,\n",
    "        'QLattice': 4,\n",
    "        'geneticengine': 3,\n",
    "        'operon': 5,\n",
    "        'pysr': 3,\n",
    "        'uDSR': 4\n",
    "    },\n",
    "    'hosp': {\n",
    "        'Bingo': 4,\n",
    "        'E2ET': 2,\n",
    "        'LassoCV': 1,\n",
    "        'PS-Tree': 3,\n",
    "        'QLattice': 5,\n",
    "        'geneticengine': 5,\n",
    "        'operon': 4,\n",
    "        'pysr': 4,\n",
    "        'uDSR':5 \n",
    "    },\n",
    "    'deaths': {\n",
    "        'Bingo': 4,\n",
    "        'E2ET': 2,\n",
    "        'LassoCV': 1,\n",
    "        'PS-Tree': 3,\n",
    "        'QLattice': 5,\n",
    "        'geneticengine': 4,\n",
    "        'operon': 4,\n",
    "        'pysr': 3,\n",
    "        'uDSR': 4\n",
    "    }\n",
    "}\n",
    "frames = []\n",
    "for k,v in expert_scores.items():\n",
    "    for alg,score in v.items():\n",
    "        data = {}\n",
    "        data['task'] = k\n",
    "        data['algorithm'] = alg\n",
    "        data['expert_score'] = score\n",
    "        frames.append(data)\n",
    "         \n",
    "expertdf = pd.DataFrame.from_records(frames)\n",
    "# display(expertdf)\n",
    "df_final = df_best.merge(expertdf, on=['task','algorithm'])\n",
    "len(df_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5648af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "expertdf.groupby('algorithm').mean().round(3).sort_values(by='expert_score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b82be7a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfm = df_final[['algorithm','task','r2_test','simplicity','expert_score']].melt(id_vars = ['algorithm','task'])\n",
    "print('r2_test')\n",
    "display(\n",
    "    dfm.loc[dfm.variable=='r2_test'].groupby(['algorithm','task'])['value'].median().round(2).unstack()\n",
    ")\n",
    "print('simplicity')\n",
    "display(\n",
    "    dfm.loc[dfm.variable=='simplicity'].groupby(['algorithm','task'])['value'].median().round(2).unstack()\n",
    ")\n",
    "print('expert_score')\n",
    "display(\n",
    "    dfm.loc[dfm.variable=='expert_score'].groupby(['algorithm','task'])['value'].median().round(2).unstack()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66be921f",
   "metadata": {},
   "source": [
    "# calculate metric ranks\n",
    "\n",
    "calculate trust score, defined as harmonic mean of r2, simplicity, and expert score. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d92ed372",
   "metadata": {},
   "outputs": [],
   "source": [
    "mets = ['r2_test','simplicity','expert_score']\n",
    "rank_mets = []\n",
    "df_sum = (df_final.groupby(['algorithm','task'],as_index=False)\n",
    "          [mets]\n",
    "#           .mean()\n",
    "          .max()\n",
    "         )\n",
    "n_algs = df_final.algorithm.nunique()\n",
    "df_sum['r2_test'] = df_sum['r2_test'].round(3)\n",
    "# df_sum['simplicity'] = df_sum['simplicity'].round(3)\n",
    "for col in mets:\n",
    "    ascending=False\n",
    "    colname = col+'_rank' \n",
    "    df_sum[colname]=(n_algs-df_sum\n",
    "                     .groupby('task')\n",
    "                     [col]\n",
    "                     .rank(ascending=ascending, \n",
    "                           method='dense')\n",
    "                    ) \n",
    "    assert df_sum[colname].max() <= df_sum.algorithm.nunique()\n",
    "    rank_mets.append(colname)\n",
    "\n",
    "\"\"\"\n",
    "compute harmonic mean\n",
    "\"\"\"\n",
    "from scipy import stats\n",
    "trust_score = (df_sum.groupby(['task','algorithm'])[rank_mets]\n",
    "               .apply(lambda x: stats.hmean(x.values[0]))\n",
    "#                .apply(lambda x: np.mean(x.values[0]))\n",
    "               .rename('trust_score') \n",
    "              )\n",
    "df_sum = df_sum.merge(trust_score, on=['task','algorithm'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3536e0e6",
   "metadata": {},
   "source": [
    "# display winners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3456e094",
   "metadata": {},
   "outputs": [],
   "source": [
    "winners = df_sum.groupby(['algorithm'])['trust_score'].mean().sort_values(ascending=False)\n",
    "order = winners.index \n",
    "print(df_sum.groupby('algorithm').mean().loc[order]['trust_score'].round(2).reset_index().to_markdown())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6913bdb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "mets = rank_mets + ['trust_score']\n",
    "\n",
    "dfm = df_sum[['algorithm','task']+mets].melt(id_vars = ['algorithm','task'])\n",
    "dfm.groupby(['task','variable','algorithm'])['value'].max().unstack().round(2)[order]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
