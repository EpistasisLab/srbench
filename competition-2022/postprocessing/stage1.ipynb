{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2083efe1",
   "metadata": {},
   "source": [
    "## Judging Criteria for the *Synthetic* Track\n",
    "\n",
    "The following (measurable and objective) aspects are considered to evaluate the quality of an SR method:\n",
    "\n",
    "* `accuracy`: The produced SR models should be accurate. Accuracy is measured in terms of the [R2 Score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.r2_score.html?highlight=r2_score#sklearn.metrics.r2_score) measured on the test set.\n",
    "To account for the fact that methods achieving similar R2 Scores can be considered equivalently good, the R2 Score is rounded to the 3rd decimal (e.g., 0.8712 -> 0.871).\n",
    "* `simplicity`: SR models should be simpler in order to stand a higher chance of being interpretable. As a loose metric for this, the number of components of the model (post simplification via `sympy`) is considered as measure of complexity, i.e., the opposite of simplicity.\n",
    "We consider finding an accurate but less complex model to become increasingly hard with decreasing levels of complexity.\n",
    "Thefefore, complexity is actually calculated as a function of `round(-log_5(s), 1)`, where `s` is the number of components in the (`sympy`-simplified) model. This translates to the figure below, such that, e.g., a model with 4 components is considered to be better than one with 5 components, while a model with 85 components is considered to be the same of one with 90 components.\n",
    "* `property`: Each synthetic data set is prepared with respect to a certain property (e.g., presence of many irrelevant features or ability to re-discover the data-originating function).\n",
    "The models generated by a method should reflect that property (e.g., respectively, make no use of irrelevant features or be symbolically equivalent to the data-originating function).\n",
    "Since the data sets are private, so are their properties.\n",
    "However, we can disclose that frequency of discovery of the data-originating function is one of the properties.\n",
    "\n",
    "Given the aspects above, the participating methods will be evaluated as follows:\n",
    "1. Each method is run 10 times, producing 10 models, for each data set.\n",
    "\n",
    "1. For each data set, the `accuracy`, `simplicity`, and `property` of each SR method are measured. The first two are the same across each data set, while the latter is data set-specific.\n",
    "For each aspect, the median of the 10 repetitions is considered.\n",
    "\n",
    "2. Independently for each data set, the participating methods are ranked in terms of the four aspects. Higher rank = better performance.\n",
    "\n",
    "2. Next, a data set-specific score is computed across the three aspects, by taking the harmonic mean of the respective ranks (i.e., `score_dataset = harmonic_mean(rank_accuracy, rank_simplicity, rank_property)`); this promotes methods producing models that have a good trade-off between the different aspects (e.g., an algorithm that produces most-accurate but very complex models will score worse than an algorithm that produces decently-accurate and not too complex models).\n",
    "\n",
    "3. A final score is obtained by averaging across data set-specific scores: `final_score = mean_i(score_dataset_i)`.\n",
    "\n",
    "The algorithm with highest final score wins this track of the competition.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3297d66",
   "metadata": {},
   "source": [
    "# Winners\n",
    "\n",
    "\n",
    "|    | algorithm     |   hmean_score |\n",
    "|---:|:--------------|--------------:|\n",
    "|  1 | QLattice      |          6.23 |\n",
    "|  2 | pysr          |          5.26 |\n",
    "|  3 | uDSR          |          5.67 |\n",
    "|  4 | operon        |          4.38 |\n",
    "|  5 | Bingo         |          4.32 |\n",
    "|  6 | E2ET          |          2.74 |\n",
    "|  7 | geneticengine |          2.54 |\n",
    "|  8 | eql           |          1.33 |\n",
    "|  9 | PS-Tree       |          0.85 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f1e4ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from glob import glob\n",
    "from pathlib import Path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f8b7f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdir = '../results_stage1/'\n",
    "datadir = '../experiment/data/stage1/data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4778c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdb\n",
    "frames = []\n",
    "i = 0\n",
    "for f in Path(rdir).rglob('*.json'):\n",
    "#     print(f)\n",
    "    if 'gpzgd' in str(f):\n",
    "        continue\n",
    "    if 'seir' in str(f):\n",
    "        continue\n",
    "#     if 'easier' in str(f):\n",
    "#         continue\n",
    "    with open(f, 'r') as of:\n",
    "        d = json.load(of)\n",
    "    if 'symbolic_equivalence' in d.keys():\n",
    "        d['solution'] = int(d['symbolic_equivalence']['equivalent'])\n",
    "        d['pred_model'] = d['symbolic_equivalence']['pred_model']\n",
    "        d['sym_diff'] = d['symbolic_equivalence']['sym_diff']\n",
    "        d['sym_frac'] = d['symbolic_equivalence']['sym_frac']\n",
    "        try:\n",
    "            d['true_model'] = d['symbolic_equivalence']['true_model']\n",
    "        except Exception as e:\n",
    "            pdb.set_trace()\n",
    "        d['filename'] = f\n",
    "    d['dataset'] = d['dataset'].replace('_base','_easy')\n",
    "    del d['params']\n",
    "#     if d['algorithm'] == 'Bingo' and 'seirdR' in d['dataset'] and 'easy' in d['dataset']:\n",
    "#         print(d['r2_test'])\n",
    "#         pdb.set_trace()\n",
    "    frames.append(d)\n",
    "    i += 1\n",
    "    \n",
    "print('loaded',i,'results')\n",
    "df = pd.DataFrame.from_records(frames)\n",
    "\n",
    "# fix cutoff dataset names\n",
    "df['dataset'] = df['dataset'].apply(lambda x: x+'ata' if x.endswith('_d') else x)\n",
    "\n",
    "########################################\n",
    "# metric cleanup\n",
    "# df['r2_test'] = df['r2_test'].round(5)\n",
    "df['mse_test'] = df['mse_test'].round(3)\n",
    "df['simplicity'] = (df['simplicity']-df['simplicity'].min())/(df['simplicity'].max()-df['simplicity'].min())\n",
    "########################################\n",
    "# get dataset sizes\n",
    "dataset_nsamples = {}\n",
    "dataset_nfeatures = {}\n",
    "    \n",
    "for d in df.dataset.unique():\n",
    "    filename = datadir+d+'.csv'\n",
    "    if not os.path.exists(filename):\n",
    "        filename = datadir+'seir/'+d+'.csv'\n",
    "    tmp = pd.read_csv(filename)\n",
    "    dataset_nsamples[d] = len(tmp)\n",
    "    dataset_nfeatures[d] = tmp.shape[1]-1\n",
    "\n",
    "ns = pd.DataFrame({'dataset':dataset_nsamples.keys(),\n",
    "              'nsamples':dataset_nsamples.values(),\n",
    "             })\n",
    "nf = pd.DataFrame({'dataset':dataset_nfeatures.keys(),\n",
    "              'nfeatures':dataset_nfeatures.values(),\n",
    "             })\n",
    "data = pd.merge(ns,nf,on='dataset')\n",
    "df = df.merge(data,on='dataset')\n",
    "df['nsize'] = df['nsamples'].apply(lambda x: '>10,000' if x>10000 else '>1000' if x>1000 else '<=1000')\n",
    "df['fsize'] = df['nfeatures'].apply(lambda x: '>=1000' if x>=1000 else '>=100' if x>=100 else '<100')\n",
    "########################################\n",
    "# time transform\n",
    "df['time_hr'] = df['time_time']/3600\n",
    "df['time_mins'] = df['time_time']/60\n",
    "########################################\n",
    "# merge dataset names\n",
    "df['dataset-full'] = df['dataset'].copy()\n",
    "df['dataset'] = df['dataset'].apply(lambda x: '_'.join(x.split('_')[1:]))\n",
    "df['task'] = df['dataset'].apply(lambda x: x.split('_')[0] if 'exact' not in x else '_'.join(x.split('_')[:2]))\n",
    "df['difficulty'] = df['dataset'].apply(lambda x: x.split('_')[1] if 'exact' not in x else x.split('_')[2])\n",
    "df['difficulty'] = df['difficulty'].apply(lambda x: {'easier':'0-easier','easy':'1-easy','medium':'2-medium','hard':'3-hard'}[x])\n",
    "df['random_state'] = df['dataset-full'].apply(lambda x: x.split('_')[0])\n",
    "# df.loc[df.difficulty=='base','difficulty'] = 'easy'\n",
    "# df['difficulty'] = df['difficulty'].astype('category')\n",
    "# df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35b44ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[(df.algorithm=='Bingo') \n",
    "       & (df.task=='seirdR')\n",
    "       & (df.difficulty=='easy')\n",
    "      ]['r2_test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d0b70e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85750abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.task.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6439c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "METRICS = [\n",
    "   'r2_test', \n",
    "   'feature_absence_score',\n",
    "   'solution',\n",
    "   'simplicity'\n",
    "]\n",
    "\n",
    "def task_property(d):\n",
    "    if 'seir' in d or 'exact_formula' in d:\n",
    "        return 'solution'\n",
    "    elif 'localopt' in d or 'featureselection' in d:\n",
    "        return 'feature_absence_score'\n",
    "    else:\n",
    "        return 'r2_test'\n",
    "\n",
    "df['property'] = df['task'].apply(task_property)\n",
    "df[['task','property']].drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a1bb1ba",
   "metadata": {},
   "source": [
    "# check run completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e29af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop_duplicates(subset=['dataset-full','algorithm'])\n",
    "tmp = df.groupby(['task','difficulty','algorithm'])['random_state'].count().unstack()\n",
    "display(\n",
    "    tmp.fillna('DNF')\n",
    ")\n",
    "print(tmp.mean(),'runs completed on average')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc49be0c",
   "metadata": {},
   "source": [
    "# summarize scores  and calculate rankings\n",
    "\n",
    "- mean of trials are taken as the metric score for each method on each dataset. \n",
    "- ranks are calculated on a per dataset basis, using this mean score. \n",
    "- ranks are calculated for `r2_test_rounded`, `simplicity`, and a third property depending on dataset. \n",
    "- the harmonic mean of the rankings over all problems is the method's final score. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f8f8a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['dataset','property']]\n",
    "df['property_score'] = df.apply(lambda x: x[x['property']], axis=1)\n",
    "mets = ['r2_test','simplicity','property_score']\n",
    "rank_mets = []\n",
    "df_sum = (df.groupby(['algorithm','task','difficulty','dataset'],as_index=False)\n",
    "          [mets]\n",
    "          .mean()\n",
    "         )\n",
    "n_algs = df.algorithm.nunique()\n",
    "df_sum['r2_test'] = df_sum['r2_test'].round(3)\n",
    "# df_sum['simplicity'] = df_sum['simplicity'].round(3)\n",
    "for col in mets:\n",
    "#     ascending = 'r2' not in col and 'simplicity' not in col and 'feature_absence' not in col\n",
    "    ascending=False\n",
    "    colname = col+'_rank' \n",
    "    df_sum[colname]=(n_algs-df_sum\n",
    "                     .groupby('dataset')\n",
    "                     [col]\n",
    "                     .rank(ascending=ascending, \n",
    "                           method='dense')\n",
    "                    ) \n",
    "    assert df_sum[colname].max() <= df_sum.algorithm.nunique()\n",
    "    rank_mets.append(colname)\n",
    "\n",
    "\"\"\"\n",
    "compute harmonic mean\n",
    "\"\"\"\n",
    "from scipy import stats\n",
    "# df_sum['hmean_score'] = df.groupby('dataset')[rank_mets].agg(stats.hmean)\n",
    "for task in ['exact_formula']:\n",
    "    display(df_sum.loc[df_sum.task.str.contains(task)].groupby(['task','difficulty','algorithm']).median())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3703bdb2",
   "metadata": {},
   "source": [
    "# harmonic mean "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6252a38d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "# df_sum['hmean_score'] = \n",
    "import pdb\n",
    "max_rank = df_sum.algorithm.nunique()\n",
    "def nanhmean(x):\n",
    "    x[x.isna()]=max_rank\n",
    "    assert len(x.values)==1\n",
    "    return stats.hmean(x.values[0])\n",
    "    \n",
    "hmean_score = df_sum.groupby(['dataset','algorithm'])[rank_mets].apply(nanhmean).rename('hmean_score') #.apply(lambda x: 0.0 if np.isnan(x) else x) #.apply(stats.hmean)\n",
    "df_sumh = df_sum.merge(hmean_score, on=['dataset','algorithm'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e2b39f9",
   "metadata": {},
   "source": [
    "# calculate winners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "582d48cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "winners = df_sumh.groupby(['algorithm'])['hmean_score'].mean().sort_values(ascending=False)\n",
    "order = winners.index \n",
    "winners.reset_index().round(2).to_markdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af3eae0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "winners.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22522691",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sum.groupby(['dataset','algorithm'])[rank_mets].mean().unstack().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d666ba3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_plt = df_sumh.melt(id_vars = ['dataset','algorithm','task','difficulty'])\n",
    "# df_plt = df_sumh.melt(id_vars = ['algorithm','task'])\n",
    "df_plt = df_plt.loc[df_plt.variable.isin(['hmean_score']+rank_mets)]\n",
    "g = sns.catplot(\n",
    "    kind='point',\n",
    "    join=False,\n",
    "    palette='flare_r',\n",
    "    data=df_plt,\n",
    "    y='algorithm',\n",
    "    order=order,\n",
    "    x='value',\n",
    "    col='variable',\n",
    "    col_order = ['hmean_score']+rank_mets,\n",
    "#     col_wrap=3,\n",
    "    sharex=False,\n",
    "    aspect=0.65,\n",
    "    height=6\n",
    ")\n",
    "g.set(xlabel='Better ->',ylabel='')\n",
    "for k,ax in g.axes_dict.items():\n",
    "    ax.grid(axis='y')\n",
    "    if 'hmean' not in k:\n",
    "        ttl = ax.get_title()[11:]\n",
    "    else:\n",
    "        ttl = 'Overall Score (Hamonic Mean)'\n",
    "#         ttl = ax.get_title()[11:]\n",
    "    ax.set_title(ttl.replace('_',' ').title())\n",
    "# sns.despine(left=True,bottom=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e328c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d1e5a23",
   "metadata": {},
   "source": [
    "# scores and rankings for each dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ede0611",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tbl = df_sumh.melt(id_vars = ['dataset','algorithm','task','difficulty'],var_name='metric')\n",
    "# df_sumh.groupby(['task','difficulty','algorithm'])[rank_mets+['hmean_score']].mean()\n",
    "import warnings\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    for task, dfg in df_tbl.groupby('task'):\n",
    "        task_score = task_property(task)\n",
    "        print(10*'=',task,10*'=')\n",
    "        task_metrics = ['r2_test','simplicity','property_score']\n",
    "        task_metrics = [tm+'_rank' for tm in task_metrics] + ['hmean_score']\n",
    "#         task_metrics.extend([tm+'_rank' for tm in task_metrics])\n",
    "        dfp = dfg.loc[dfg.metric.isin(task_metrics)]\n",
    "    #     dfg.loc[dfg.metric.str.contains('rank'),'metric'] = dfg.loc[dfg.metric.str.contains('rank'),'metric'].round(1)\n",
    "        dfp.loc[dfp.metric=='property_score','metric'] = task_score\n",
    "        dfp.loc[dfp.metric=='property_score_rank','metric'] = task_score+'_rank'\n",
    "        rdr = [o for o in order if o in dfg['algorithm'].unique()]\n",
    "        display(dfp.groupby(['task','difficulty','algorithm','metric'])['value'].median().unstack().round(1)) #[rdr].round(2))\n",
    "\n",
    "#         task_metrics = [tm+'_rank' for tm in task_metrics]\n",
    "#         dfp = dfg.loc[dfg.metric.isin(task_metrics)]\n",
    "#         dfp.loc[dfp.metric=='property_score','metric'] = task_score\n",
    "#         dfp.loc[dfp.metric=='property_score_rank','metric'] = task_score+'_rank'\n",
    "#         display(dfp.groupby(['task','difficulty','metric','algorithm'])['value'].median().unstack()[rdr])\n",
    "        print(40*'=')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c40b3943",
   "metadata": {},
   "outputs": [],
   "source": [
    "metr = ['r2_test_rank','simplicity_rank','property_score_rank','hmean_score']\n",
    "(df_tbl.loc[df_tbl.metric.isin(metr)]\n",
    " .groupby(['task','metric','algorithm'])['value']\n",
    " .mean()\n",
    " .unstack()\n",
    " .round(1)[order]\n",
    " .fillna('DNF')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7dd7c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "(df_tbl.loc[df_tbl.metric=='hmean_score']\n",
    " .groupby(['task','difficulty','metric','algorithm'])['value']\n",
    " .mean()\n",
    " .unstack()\n",
    " .round(1)[order]\n",
    " .fillna('DNF')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc9c2335",
   "metadata": {},
   "source": [
    "# explore solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5059ca86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sympy as sp\n",
    "# define local namespace dictionary\n",
    "SEIR_VARS = {}\n",
    "for s in ['dS', 'dE', 'dI', 'dR',\n",
    "          'x_S', 'x_E', 'x_I', 'x_R', 'x_rzero', 'x_gamma', 'x_sigma', 'x_c', 'x_N', 'x_Aux']:\n",
    "    SEIR_VARS.update({ s: sp.Symbol(s) })\n",
    "        \n",
    "EF_VARS = {f'x{i}':sp.Symbol(f'x{i}') for i in range(5)}\n",
    "LD = dict(seir = SEIR_VARS, ef=EF_VARS)\n",
    "frames = []\n",
    "dfg = df.loc[df.task=='exact_formula']\n",
    "for (task,difficulty),dfgpg in dfg.groupby(['task','difficulty']):\n",
    "    print(40*'=')\n",
    "    print(task,difficulty)\n",
    "    print('true_model:') #,true_model)\n",
    "    local_dict = LD['seir'] if 'seir' in task else LD['ef']\n",
    "    true_model = sp.parse_expr(dfgpg['true_model'].values[0], local_dict=local_dict)\n",
    "    display(true_model)\n",
    "    print(40*'=')\n",
    "    for (alg,task,difficulty),dfz in dfgpg.groupby(['algorithm','task','difficulty']):\n",
    "        row = dfz.loc[dfz['r2_test'].idxmax()]\n",
    "        if len(row['pred_model']) > 1000:\n",
    "            continue\n",
    "        print(20*'-')\n",
    "        for c in ['algorithm','r2_test','simplicity','solution','dataset-full']: #,'filename']:\n",
    "            print(c,':',row[c],',',end=' ') #,'symbolic_model','pred_model','true_model']])\n",
    "        print('\\n')\n",
    "        pred_model = sp.parse_expr(row['pred_model'], local_dict=local_dict)\n",
    "        orig_model = sp.parse_expr(row['symbolic_model'], local_dict=local_dict)\n",
    "#         print(row['filename'])\n",
    "        if str(pred_model)=='0':\n",
    "            print('symbolic_model:')\n",
    "            display(orig_model)\n",
    "        else:\n",
    "            print('pred_model:')\n",
    "            display(pred_model)\n",
    "            print('sym_diff',row['sym_diff'])\n",
    "            print('sym_frac',row['sym_frac'])\n",
    "            print('original model:',orig_model)\n",
    "        print(20*'-')\n",
    "    print(40*'=')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "061509cd",
   "metadata": {},
   "source": [
    "# plot individual task results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adde907f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdb\n",
    "metrics = ['r2_test','solution','feature_absence_score','simplicity']\n",
    "rank_metrics = [m+'_rank' for m in metrics]\n",
    "i = 0\n",
    "for task,dft in df.groupby('task'):\n",
    "#     task_metrics = ['mse_test_rank','simplicity_rank',task_property(task)+'_rank'] \n",
    "    task_metrics = ['r2_test','simplicity',task_property(task)] \n",
    "    df_plt = dft.melt(id_vars = ['dataset','algorithm','difficulty','random_state'])\n",
    "    df_plt = df_plt.loc[df_plt.variable.isin(task_metrics)]\n",
    "    \n",
    "    if task == 'exact_formula':\n",
    "        hue_order=['0-easier','1-easy','2-medium','3-hard']\n",
    "    else:\n",
    "        hue_order=['1-easy','2-medium','3-hard']\n",
    "        \n",
    "    g = sns.catplot(\n",
    "        kind='point',\n",
    "        join=False,\n",
    "        palette='flare_r',\n",
    "        data=df_plt,\n",
    "        y='algorithm',\n",
    "        order=order,\n",
    "        x='value',\n",
    "        col='variable',\n",
    "        hue='difficulty',\n",
    "        hue_order=hue_order,\n",
    "        sharex=False,\n",
    "#         aspect=0.65,\n",
    "#         height=6\n",
    "    )\n",
    "    g.set(ylabel='', xlabel='better ->')\n",
    "    \n",
    "\n",
    "    for k,ax in g.axes_dict.items():\n",
    "        ax.grid(axis='y')\n",
    "        ax.set_title(ax.get_title()[11:].title())\n",
    "#         if 'mse' in k:\n",
    "#             ax.set_xscale('log')\n",
    "        if k=='r2_test':\n",
    "            x = df_plt.loc[df_plt.variable==k]['value']\n",
    "            x_left = ax.get_xlim()[0]\n",
    "            if x_left < x.quantile():\n",
    "#             if x_bottom < -1:\n",
    "                ax.set_xlim(x.quantile(.1), 1.0)\n",
    "    ax = g.axes.flat[0]\n",
    "    ax.text(s=task,\n",
    "#             x=0, y=1,\n",
    "            x=ax.get_xlim()[0],\n",
    "            ha='right',\n",
    "            y=ax.get_ylim()[1],\n",
    "            fontsize='large',\n",
    "            color='w',\n",
    "            backgroundcolor='k'\n",
    "           )\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
