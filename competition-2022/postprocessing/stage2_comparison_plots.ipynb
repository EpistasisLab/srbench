{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f1e4ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from glob import glob\n",
    "from pathlib import Path\n",
    "import sympy as sp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f8b7f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdir = '../results_stage2_chunk/'\n",
    "datadir = '../experiment/data/stage2/data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4778c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = []\n",
    "i = 0\n",
    "for f in Path(rdir).rglob('*.json'):\n",
    "#     print(f)\n",
    "    if '7_7' in str(f):\n",
    "        continue\n",
    "    if '135' not in str(f):\n",
    "        continue\n",
    "    if 'gpzgd' in str(f):\n",
    "        continue\n",
    "    with open(f, 'r') as of:\n",
    "        d = json.load(of)\n",
    "    frames.append(d)\n",
    "    i += 1\n",
    "    \n",
    "print('loaded',i,'results')\n",
    "df = pd.DataFrame.from_records(frames)\n",
    "\n",
    "# hide linear regression name\n",
    "df.loc[df.algorithm=='LinearRegression','algorithm'] = 'Mamba'\n",
    "# fix cutoff dataset names\n",
    "df['dataset'] = df['dataset'].apply(lambda x: x+'ata' if x.endswith('_d') else x)\n",
    "########################################\n",
    "# normalize simplicity\n",
    "df['simplicity'] = (df['simplicity']-df['simplicity'].min())/(df['simplicity'].max()-df['simplicity'].min())\n",
    "########################################\n",
    "# time transform\n",
    "df['time_hr'] = df['time_time']/3600\n",
    "df['time_mins'] = df['time_time']/60\n",
    "########################################\n",
    "# deconstruct dataset names\n",
    "df['dataset-full'] = df['dataset'].copy()\n",
    "df['dataset'] = df['dataset'].apply(lambda x: '_'.join(x.split('_')[1:-1]))\n",
    "df['task'] = df['dataset'].apply(lambda x: x.split('value_')[-1].split('_')[0])\n",
    "df['horizon'] = df['dataset'].apply(lambda x: 7 if '7' in x else 14)\n",
    "df['random_state'] = df['dataset-full'].apply(lambda x: x.split('_')[0])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6439c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "METRICS = [\n",
    "   'mse_train', \n",
    "   'mae_train', \n",
    "   'r2_train',\n",
    "   'mse_test', \n",
    "   'mae_test', \n",
    "   'r2_test', \n",
    "   'accuracy', \n",
    "   'feature_absence_score' \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6749c9cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34531f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['task'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a1bb1ba",
   "metadata": {},
   "source": [
    "# check run completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e29af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(['dataset','algorithm'])['random_state'].count().unstack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58146f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "order = df.groupby(['algorithm'])['r2_test'].mean().sort_values(ascending=False).index\n",
    "df.groupby(['task','horizon','algorithm'])['r2_test'].median().unstack().round(3)[order]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a2befed",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(['algorithm'])['r2_test'].median().sort_values(ascending=False).round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb62bc54",
   "metadata": {},
   "outputs": [],
   "source": [
    "order = df.groupby(['algorithm'])['simplicity'].mean().sort_values(ascending=False).index\n",
    "df.groupby(['task','horizon','algorithm'])['simplicity'].mean().unstack().round(3)[order]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f104a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.groupby(['algorithm'])['simplicity'].mean().sort_values(ascending=False).round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff341499",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_sel\n",
    "import pdb\n",
    "median_scores= df.groupby(['task','horizon','algorithm'])['r2_test'].median().unstack()\n",
    "sel_model = {}\n",
    "frames = []\n",
    "for idx, row in median_scores.iterrows():\n",
    "#     task = row['task'], row['horizon'], row['algorithm'], \n",
    "    for alg in row.index:\n",
    "        if np.isnan(row[alg]): continue\n",
    "        dfg = df.loc[\n",
    "            (df.task==idx[0])\n",
    "            & (df.horizon==idx[1])\n",
    "            & (df.algorithm==alg)\n",
    "        ]\n",
    "        entry = dfg.loc[(dfg.r2_test-row.loc[alg]).abs().idxmin()]\n",
    "        assert isinstance(entry, pd.Series)\n",
    "        frames.append(entry.to_dict())\n",
    "df_best = pd.DataFrame.from_records(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ecc3d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_best.groupby('algorithm').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a4e588",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_best.loc[df_best.algorithm=='QLattice','symbolic_model']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8898375",
   "metadata": {},
   "outputs": [],
   "source": [
    "# redo models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d61c9cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluation import get_symbolic_model, simplicity, round_floats\n",
    "\n",
    "def redo_model(x):\n",
    "    seed = x['random_state'] #.values[0]\n",
    "    ds = x['dataset'] #.values[0]\n",
    "    task = x['task'] #.values[0]\n",
    "    dataset = pd.read_csv(f'../experiment/data/stage2/data/{seed}_{ds}_train.csv')\n",
    "#     '../experiment/data/stage2/data/'\n",
    "#     print(f'../experiment/data/stage2/data/{seed}_{ds}_{task}_train.csv')\n",
    "    feature_names = [k for k in dataset.columns if k!= task]\n",
    "    local_dict = {k:sp.Symbol(k) for k in feature_names}\n",
    "    if x['algorithm'] == 'QLattice':\n",
    "        print('symbolic_model:',\n",
    "              x.symbolic_model)\n",
    "    mdl = str(get_symbolic_model(x.symbolic_model, local_dict=local_dict, simplify=False))\n",
    "    simp = simplicity(mdl,feature_names,simplify=False)\n",
    "    if x['algorithm'] == 'QLattice':\n",
    "        print('get_symbolic_model:',\n",
    "              mdl)\n",
    "    x['simplified_model'] = round_floats(mdl, 6)\n",
    "    x['simplicity'] = simp\n",
    "#     if x['algorithm'] == 'QLattice': pdb.set_trace()\n",
    "    return x\n",
    "   \n",
    "df_best = df_best.transform(lambda x: redo_model(x), axis=1) \n",
    "# normalize simplicity\n",
    "df_best['simplicity'] = (df_best['simplicity']-df_best['simplicity'].min())/(df_best['simplicity'].max()-df_best['simplicity'].min())\n",
    "\n",
    "df_best[['task','simplified_model','simplicity','algorithm']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb6e336",
   "metadata": {},
   "source": [
    "# TODO\n",
    "- for each dataset \n",
    "    - for each algorithm\n",
    "        - report R2, mse, simplicity\n",
    "        - report simplified_model\n",
    "        - plot test predictions versus real data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9dbda25",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfm = df_best[['algorithm','task','r2_test','simplicity']].melt(id_vars = ['algorithm','task'])\n",
    "display(\n",
    "    dfm.loc[dfm.variable=='r2_test'].groupby(['algorithm','task'])['value'].median().round(2).unstack()\n",
    ")\n",
    "display(\n",
    "    dfm.loc[dfm.variable=='simplicity'].groupby(['algorithm','task'])['value'].median().round(2).unstack()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37db8032",
   "metadata": {},
   "outputs": [],
   "source": [
    "%config InlineBackend.figure_format ='retina'\n",
    "from sklearn.metrics import r2_score\n",
    "import sympy as sp \n",
    "import pdb \n",
    "from matplotlib import rc\n",
    "from evaluation import get_symbolic_model, round_floats\n",
    "rc('text', usetex=True)\n",
    "rc('font', family='serif')\n",
    "plt.rcParams.update({\"text.latex.preamble\":r'\\usepackage{tabularx} '})\n",
    "\n",
    "def comparison_plot(task,horizon,alg,dfg):\n",
    "    \"\"\"Comparison plot of model for report.\"\"\"\n",
    "    seed = dfg['random_state'].values[0]\n",
    "    ds = dfg['dataset'].values[0]\n",
    "#     '../experiment/data/stage2/data/'\n",
    "    print(f'../experiment/data/stage2/data/{seed}_{ds}_{task}_train.csv')\n",
    "    df1 = pd.read_csv(f'../experiment/data/stage2/data/{seed}_{ds}_train.csv')\n",
    "    df1['fold'] = 'train'\n",
    "    df2 = pd.read_csv(f'../experiment/data/stage2/data/{seed}_{ds}_test.csv')\n",
    "    df2['fold'] = 'test'\n",
    "    df = df1.append(df2)\n",
    "    df['date'] = df['date'].astype(np.datetime64)\n",
    "    dataset = df.set_index('date')\n",
    "    feature_names = [k for k in dataset.columns if k != task]\n",
    "    \n",
    "    dfp = dfg.copy()\n",
    "    y_true = np.asarray(dfp.y_true_test.values[0])\n",
    "    y_pred = np.asarray(dfp.y_pred_test.values[0])\n",
    "    dates = np.asarray([np.datetime64(x) for x in dfp.idx_test.values[0]])\n",
    "    idx = np.argsort(dates)\n",
    "    \n",
    "#     y_true_train = np.asarray(dfp.y_true_train.values[0])\n",
    "    y_true_train = df1['value_'+task].values\n",
    "    y_pred_train = np.asarray(dfp.y_pred_train.values[0])\n",
    "    dates_train = np.asarray([np.datetime64(x) for x in dfp.idx_train.values[0]])\n",
    "    idx_train = np.argsort(dates_train)\n",
    "    \n",
    "    dates_all = np.concatenate((dates_train, dates))\n",
    "    idx_all = np.argsort(dates_all)\n",
    "    y_true_all = np.concatenate((y_true_train, y_true))\n",
    "    y_pred_all = np.concatenate((y_pred_train, y_pred))\n",
    "    ########## \n",
    "    h = plt.figure(figsize=(12,8))\n",
    "#     plt.suptitle(f'{horizon} day prediction of {task} (Method: {alg})')\n",
    "    \n",
    "#     ax = h.add_subplot(221)\n",
    "#     plt.subplot(121)\n",
    "    plt.subplot(223)\n",
    "#     plt.subplot(325)\n",
    "#     plt.plot(dates_train[idx_train],y_pred_train[idx_train],'x',color='gray',label='y\\_pred\\_train')\n",
    "    plt.plot(dates_train[idx_train],y_true_train[idx_train],'x',color='gray',alpha=0.5,label='y\\_true\\_train')\n",
    "#     plt.plot(dates_all[idx_all],y_true_all[idx_all],'.',color='r',alpha=0.3,label='y\\_true\\_all')\n",
    "    \n",
    "    plt.plot(dates[idx],y_true[idx],'xr',label='y\\_true\\_test')\n",
    "    plt.plot(dates_all[idx_all],y_pred_all[idx_all],'-',color='b',label='y\\_pred\\_all')\n",
    "#     plt.plot(dates[idx],y_pred[idx],'+b',label='y\\_pred')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Count')\n",
    "    plt.legend()\n",
    "    plt.title('Predictions on Test Set')\n",
    "    \n",
    "#     ax = h.add_subplot(122)\n",
    "    plt.subplot(224)\n",
    "#     plt.subplot(326)\n",
    "#     error = y_true-y_pred\n",
    "    plt.plot(y_pred,y_true,'.b',label=  f'$R^2$={r2_score(y_true, y_pred):.3f}')\n",
    "    plt.ylabel('y\\_true')\n",
    "    plt.xlabel('y\\_pred')\n",
    "    plt.plot([y_true.min(),y_true.max()],[y_true.min(),y_true.max()],'--k') \n",
    "    plt.legend()\n",
    "    \n",
    "#     plt.subplot(311)\n",
    "    plt.subplot(211)\n",
    "    ax = plt.gca()\n",
    "#     bbox = ax.get_tightbbox(h.canvas.get_renderer())\n",
    "    mdl = dfg.simplified_model.values[0]\n",
    "    mdl = mdl.replace('value_','')\n",
    "    simp = dfg.simplicity.values[0]\n",
    "#     simp = simplicity(str(mdl), feature_names)\n",
    "    max_size=1000\n",
    "    max_w=700\n",
    "#     if isinstance(mdl,float):\n",
    "#         mdl = dfg.symbolic_model.values[0]\n",
    "        \n",
    "    if len(mdl) < max_size: \n",
    "# #         print('before parsing:',mdl)\n",
    "        mdl = sp.parse_expr(mdl)\n",
    "#         print('before rounding:',mdl)\n",
    "        tmp = round_floats(mdl,dec=5)\n",
    "        tmp = round_floats(tmp,dec=5)\n",
    "#         print('after rounding:',tmp)\n",
    "        if not isinstance(tmp,float):\n",
    "            mdl = tmp\n",
    "#             print('changes accepted')\n",
    "        mdl = sp.latex(mdl, \n",
    "                       fold_short_frac = True, \n",
    "    #                    mode='inline',\n",
    "    #                        max=3\n",
    "                      )\n",
    "    #     if len(mdl) > max_size:\n",
    "    #         mdl = mdl[:max_size] + ' \\cdots $'\n",
    "        mdl = task + ' = ' + mdl\n",
    "        mdl = (\n",
    "#                r'\\begin{tabularx}{'+str(max_w) + r'pt}{p{'+str(max_w) + r'pt}} '\n",
    "    #            r'\\begin{tabular}{p{20pt}} '\n",
    "               '$' + mdl + '$'\n",
    "#                +r' \\end{tabularx}'\n",
    "              )\n",
    "    \n",
    "    else: \n",
    "        mdl = 'black-box'\n",
    "        \n",
    "    print(mdl,'len:',len(mdl))\n",
    "    dfx = pd.DataFrame({\n",
    "                       'Task':f'Predict {task}',\n",
    "                       'Horizon':f'{horizon} days',\n",
    "                       'Algorithm':alg,\n",
    "                       'R2 Score':dfg.r2_test.values[0],\n",
    "                       'Simplicity':simp,\n",
    "                       'Model':mdl,\n",
    "#                        'Model':'',\n",
    "                      },\n",
    "                      index=[0]\n",
    "                     ).round(3) #.T #.rest_index()\n",
    "    \n",
    "    ax.axis('off')\n",
    "    table = plt.table(cellText=dfx.values.T, \n",
    "                      rowLabels=dfx.columns, \n",
    "#                       loc='upper center', \n",
    "#                       loc='bottom', \n",
    "                      bbox=[0.0,0.0,1.0,0.7],\n",
    "#                       bbox=bbox,\n",
    "                      rowLoc='right',\n",
    "                      cellLoc='left',\n",
    "                      edges='open',\n",
    "                      colWidths = [1],\n",
    "                     ) \n",
    "    table.auto_set_font_size(False)\n",
    "    table.set_fontsize(16)\n",
    "    for key,cell in table._cells.items():\n",
    "        if key == (5,0):\n",
    "#             print('old model cell height:',cell._height)\n",
    "            cell._height *= 2\n",
    "#             print('new model cell height:',cell._height)\n",
    "#             cell._size=11\n",
    "            if len(mdl) < 200:\n",
    "                cell.set_fontsize(14)\n",
    "            elif len(mdl) < 500:\n",
    "                cell.set_fontsize(10)\n",
    "            elif len(mdl) < 1000:\n",
    "                cell.set_fontsize(9)\n",
    "            else:\n",
    "                cell.set_fontsize(8)\n",
    "        else:\n",
    "            ahh = cell._height\n",
    "#             cell._size=14\n",
    "#             print('cell height:',h)\n",
    "            cell._height = 4/5*ahh\n",
    "    h.tight_layout()\n",
    "    for ext in ['.pdf','.png']:\n",
    "        h.savefig(f'figs/{task}_{horizon}_{alg}_summary.{ext}', dpi=300,\n",
    "                 bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b43274b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "for (task,horizon,alg),dfg in df_best.groupby(['task','horizon','algorithm']):\n",
    "#     if i == 9: break\n",
    "    i+=1\n",
    "    if horizon == 7: continue\n",
    "    comparison_plot(task,horizon,alg,dfg)\n",
    "#     mdl = dfg.simplified_model.values[0]\n",
    "#     mdl = sp.parse_expr(mdl)\n",
    "#     mdl = sp.latex(mdl)\n",
    "#     display(mdl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3acf3ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sympy import init_printing\n",
    "# from sympy.printing.dot import dotprint\n",
    "# import warnings\n",
    "# pd.options.display.max_colwidth = 2000\n",
    "# # import graphviz\n",
    "# # init_printing()\n",
    "# # from IPython.core.display import display, HTML\n",
    "# # display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "# with warnings.catch_warnings():\n",
    "#     warnings.simplefilter('ignore')\n",
    "#     for (task,horizon,alg),dfg in df_best.groupby(['task','horizon','algorithm']):\n",
    "#         if horizon == 7: continue\n",
    "#     #     comparison_plot(task,horizon,alg,dfg)\n",
    "#         print(40*'=')\n",
    "#         print(task,horizon,alg)\n",
    "#         mdl = dfg.simplified_model.values[0]\n",
    "#         simplicity = dfg.simplicity.values[0]\n",
    "#         max_size=2000\n",
    "#         max_w=50\n",
    "#         if isinstance(mdl,float):\n",
    "#             mdl = dfg.symbolic_model.values[0]\n",
    "#         else:\n",
    "#             mdl=mdl.replace('value_','')\n",
    "#             if len(mdl) < max_size: \n",
    "#         #         print('before parsing:',mdl)\n",
    "#                 mdl = sp.parse_expr(mdl)\n",
    "#         #         print('before rounding:',mdl)\n",
    "#                 mdl = round_floats(mdl)\n",
    "#                 mdl = round_floats(mdl)\n",
    "#         #         print('after rounding:',mdl)\n",
    "#         #         mdl = sp.latex(mdl, max=3)\n",
    "#         #         mdl = f'${mdl}$'\n",
    "#         #         mdl = str(mdl)\n",
    "# #         if isinstance(mdl,float):\n",
    "# #             mdl = dfg.symbolic_model.values[0]\n",
    "\n",
    "#     #     w = max_w\n",
    "#     #     step = max_w\n",
    "#     #     while w < len(mdl):\n",
    "#     #         mdl = mdl[:w] + '\\n' + mdl[w:]\n",
    "#     #         w += step\n",
    "\n",
    "#     #     if len(mdl) > max_size:\n",
    "#     #         mdl = 'black-box'\n",
    "#         df = pd.DataFrame({'Task':f'{horizon}-day prediction of {task}',\n",
    "#                            'R2 Score':dfg.r2_test.values[0],\n",
    "#                            'Simplicity':simplicity,\n",
    "#     #                        'Model':mdl\n",
    "#                           },\n",
    "#                           index=[alg]\n",
    "#                          ).round(3).T\n",
    "#     #     display(df)\n",
    "#         display(df)\n",
    "#         print('symbolic model:')\n",
    "#         display(dfg.symbolic_model.values[0])\n",
    "#         print('model:')\n",
    "#         display(mdl)\n",
    "#     #     display(mdl)\n",
    "#         print(40*'=')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea82d22f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
